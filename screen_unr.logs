max_steps is given, it will override any value given in num_train_epochs
The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expecte
d by `BertForMaskedLM.forward`,  you can safely ignore this message.
/home/sanyog/anaconda3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use
the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 9999
  Num Epochs = 10
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 1500
  Number of trainable parameters = 26912708
{'loss': 7.5071, 'learning_rate': 2.8e-05, 'epoch': 0.64}
  7%|██▏                              | 100/1500 [49:17<11:25:48, 29.39s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and
have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 9999
  Batch size = 256
{'eval_loss': 7.260837078094482, 'eval_runtime': 1336.5022, 'eval_samples_per_second': 7.481, 'eval_steps_per_second': 0.03, 'epoch': 0.64}
{'loss': 7.2002, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.27}
 13%|████▏                          | 200/1500 [2:02:03<10:21:21, 28.68s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and
have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 9999
  Batch size = 256
{'eval_loss': 7.09873104095459, 'eval_runtime': 966.9525, 'eval_samples_per_second': 10.341, 'eval_steps_per_second': 0.041, 'epoch': 1.27}
{'loss': 7.1025, 'learning_rate': 2.4e-05, 'epoch': 1.91}
 20%|██████▏                        | 300/1500 [3:09:03<10:14:46, 30.74s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and
have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 9999
  Batch size = 256
{'eval_loss': 7.026464939117432, 'eval_runtime': 959.6204, 'eval_samples_per_second': 10.42, 'eval_steps_per_second': 0.042, 'epoch': 1.91}
{'loss': 7.02, 'learning_rate': 2.2e-05, 'epoch': 2.55}
 27%|████████▌                       | 400/1500 [4:14:38<9:41:51, 31.74s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and
have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 9999
  Batch size = 256
{'eval_loss': 6.966459274291992, 'eval_runtime': 984.3791, 'eval_samples_per_second': 10.158, 'eval_steps_per_second': 0.041, 'epoch': 2.55}
{'loss': 6.9917, 'learning_rate': 1.9999999999999998e-05, 'epoch': 3.18}
 33%|██████████▋                     | 500/1500 [5:20:19<8:09:50, 29.39s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and
have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 9999
  Batch size = 256
{'eval_loss': 6.912442684173584, 'eval_runtime': 914.3619, 'eval_samples_per_second': 10.935, 'eval_steps_per_second': 0.044, 'epoch': 3.18}
 33%|██████████▋                     | 500/1500 [5:35:33<8:09:50, 29.39s/it]Saving model checkpoint to ./tokenizer/unr/checkpoint-500
Configuration saved in ./tokenizer/unr/checkpoint-500/config.json
Model weights saved in ./tokenizer/unr/checkpoint-500/pytorch_model.bin
{'loss': 6.9142, 'learning_rate': 1.8e-05, 'epoch': 3.82}
 40%|████████████▊                   | 600/1500 [6:29:55<7:24:08, 29.61s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 9999
  Batch size = 256
{'eval_loss': 6.881888389587402, 'eval_runtime': 955.8843, 'eval_samples_per_second': 10.46, 'eval_steps_per_second': 0.042, 'epoch': 3.82}
{'loss': 6.9088, 'learning_rate': 1.6e-05, 'epoch': 4.46}
 47%|██████████████▉                 | 700/1500 [7:39:16<7:33:43, 34.03s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 9999
  Batch size = 256
{'eval_loss': 6.851372718811035, 'eval_runtime': 1002.882, 'eval_samples_per_second': 9.97, 'eval_steps_per_second': 0.04, 'epoch': 4.46}
{'loss': 6.8783, 'learning_rate': 1.4e-05, 'epoch': 5.1}
 53%|█████████████████               | 800/1500 [8:45:28<6:00:46, 30.92s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 9999
  Batch size = 256
{'eval_loss': 6.7982635498046875, 'eval_runtime': 1006.0715, 'eval_samples_per_second': 9.939, 'eval_steps_per_second': 0.04, 'epoch': 5.1}
{'loss': 6.8399, 'learning_rate': 1.2e-05, 'epoch': 5.73}
 60%|███████████████████▏            | 900/1500 [9:49:30<4:46:05, 28.61s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 9999
  Batch size = 256
{'eval_loss': 6.789822578430176, 'eval_runtime': 1016.7177, 'eval_samples_per_second': 9.835, 'eval_steps_per_second': 0.039, 'epoch': 5.73}
{'loss': 6.8319, 'learning_rate': 9.999999999999999e-06, 'epoch': 6.37}
 67%|████████████████████          | 1000/1500 [10:55:13<3:40:23, 26.45s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 9999
  Batch size = 256
{'eval_loss': 6.742796897888184, 'eval_runtime': 953.9332, 'eval_samples_per_second': 10.482, 'eval_steps_per_second': 0.042, 'epoch': 6.37}
 67%|████████████████████          | 1000/1500 [11:11:07<3:40:23, 26.45s/it]Saving model checkpoint to ./tokenizer/unr/checkpoint-1000
Configuration saved in ./tokenizer/unr/checkpoint-1000/config.json
Model weights saved in ./tokenizer/unr/checkpoint-1000/pytorch_model.bin
{'loss': 6.8269, 'learning_rate': 8e-06, 'epoch': 7.01}
 73%|██████████████████████        | 1100/1500 [12:04:00<2:38:13, 23.73s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 9999
  Batch size = 256
{'eval_loss': 6.732560157775879, 'eval_runtime': 950.5848, 'eval_samples_per_second': 10.519, 'eval_steps_per_second': 0.042, 'epoch': 7.01}
{'loss': 6.8009, 'learning_rate': 6e-06, 'epoch': 7.64}
 80%|████████████████████████      | 1200/1500 [13:19:24<2:47:37, 33.52s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 9999
  Batch size = 256
{'eval_loss': 6.711937427520752, 'eval_runtime': 961.2718, 'eval_samples_per_second': 10.402, 'eval_steps_per_second': 0.042, 'epoch': 7.64}
{'loss': 6.8005, 'learning_rate': 4e-06, 'epoch': 8.28}
 87%|██████████████████████████    | 1300/1500 [14:27:40<1:59:09, 35.75s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 9999
  Batch size = 256
{'eval_loss': 6.724514484405518, 'eval_runtime': 998.6931, 'eval_samples_per_second': 10.012, 'eval_steps_per_second': 0.04, 'epoch': 8.28}
{'loss': 6.7732, 'learning_rate': 2e-06, 'epoch': 8.92}
 93%|█████████████████████████████▊  | 1400/1500 [15:33:06<56:15, 33.75s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 9999
  Batch size = 256
{'eval_loss': 6.715437889099121, 'eval_runtime': 1049.3421, 'eval_samples_per_second': 9.529, 'eval_steps_per_second': 0.038, 'epoch': 8.92}
{'loss': 6.7667, 'learning_rate': 0.0, 'epoch': 9.55}
100%|████████████████████████████████| 1500/1500 [16:39:43<00:00, 31.37s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 9999
  Batch size = 256
{'eval_loss': 6.713953971862793, 'eval_runtime': 1023.9394, 'eval_samples_per_second': 9.765, 'eval_steps_per_second': 0.039, 'epoch': 9.55}
100%|████████████████████████████████| 1500/1500 [16:56:46<00:00, 31.37s/it]Saving model checkpoint to ./tokenizer/unr/checkpoint-1500
Configuration saved in ./tokenizer/unr/checkpoint-1500/config.json
Model weights saved in ./tokenizer/unr/checkpoint-1500/pytorch_model.bin


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./tokenizer/unr/checkpoint-1500 (score: 6.713953971862793).
{'train_runtime': 61008.9407, 'train_samples_per_second': 1.574, 'train_steps_per_second': 0.025, 'train_loss': 6.944170491536458, 'epoch': 9.55}
100%|████████████████████████████████| 1500/1500 [16:56:48<00:00, 40.67s/it]
The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 9999
  Batch size = 256
100%|███████████████████████████████████████| 40/40 [17:02<00:00, 25.57s/it]
>>> Perplexity: 807.08

