export BERT_MODEL=./uncased_L-12_H-768_A-12
export CODE=./user_code
export CORPUS=./corpus
export DATA=./data
export MODEL_STAGE1=./model.stage1
export MODEL_STAGE2=./model.stage2
export PYTHONPATH="$CODE:$PYTHONPATH"

## eng-lmn

source_lang=eng
target_lang=lmn
date_format=2024_06_09_15_01
model_desc_folder_name=cope_enattn
epoch_name=epoch_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

source_lang=eng
target_lang=lmn
older_date_format=2024_06_09_15_01
date_format=2024_06_09_16_01
model_desc_folder_name=cope_enattn
epoch_name=epoch_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

source_lang=eng
target_lang=lmn
older_date_format=2024_06_09_15_01
date_format=2024_06_09_16_03
model_desc_folder_name=cope_enattn
epoch_name=epoch_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

### eng_10K-hin_10K

source_lang=eng_10K
target_lang=hin_10K
date_format=2024_06_09_17_01
model_desc_folder_name=cope_enattn
epoch_name=epoch_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_hin_10K

source_lang=eng_10K
target_lang=hin_10K
older_date_format=2024_06_09_17_01
date_format=2024_06_09_18_01
model_desc_folder_name=cope_enattn
epoch_name=epoch_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_hin_10K

source_lang=eng_10K
target_lang=hin_10K
older_date_format=2024_06_09_17_01
date_format=2024_06_09_18_03
model_desc_folder_name=cope_enattn
epoch_name=epoch_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_hin_10K

python3 $CODE/preprocess.py --source-lang $source_lang --target-lang $target_lang \
--srcdict $DATA/$model_folder_name/$model_desc/dict.$source_lang\.txt \
--trainpref $CORPUS/$model_folder_name\/10K/train \
--validpref $CORPUS/$model_folder_name\/10K/valid \
--testpref $CORPUS/$model_folder_name\/10K/test \
--destdir $DATA/$model_folder_name/$model_desc \
--log-file log_custom_fldr/$model_folder_name/$model_desc/$date_format/preprocess_$date_format.log

CUDA_VISIBLE_DEVICES=0,1 \
nohup fairseq-train \
$DATA/$model_folder_name/$model_desc \
-s $source_lang \
-t $target_lang \
--user-dir $CODE \
--task translation_with_pretrained_encoder_model \
--model $BERT_MODEL \
--model-name bert \
--arch transformer_with_pretrained_bert_d5_emb512 \
--log-format simple --max-update=1000000 \
--share-decoder-input-output-embed \
--optimizer adam --adam-betas '(0.9, 0.99)' \
--warmup-init-lr 1e-07 --lr 5e-4 \
--lr-scheduler inverse_sqrt --warmup-updates 4000 \
--weight-decay 0.001 --criterion label_smoothed_cross_entropy \
--label-smoothing 0.5 --ddp-backend legacy_ddp --clip-norm 1.0 \
--patience 500 --batch-size 4096 --max-tokens 4096 \
--no-epoch-checkpoints --skip-invalid-size-inputs-valid-test \
--wandb-project $model_folder_name\_stage1_$model_desc_folder_name\_$epoch_name \
--fp16 --save-dir $MODEL_STAGE1/$model_folder_name/$model_desc/$date_format \
--log-file log_custom_fldr/$model_folder_name/$model_desc/train_$date_format.log \
>> log_custom_fldr/$model_folder_name/$model_desc/train_execution_$date_format.log 2>&1 &

CUDA_VISIBLE_DEVICES=4,5 \
nohup fairseq-train \
$DATA/$model_folder_name/$model_desc \
-s $source_lang \
-t $target_lang \
--user-dir $CODE \
--task translation_with_pretrained_encoder_model \
--model $BERT_MODEL \
--model-name bert \
--arch transformer_with_pretrained_bert_d5_emb512 \
--fine-tuning \
--restore-file /home1/ai21mtech12003/recycle_bert/model.stage1/$model_folder_name\/$model_desc\/$older_date_format\/checkpoint_best.pt \
--reset-lr-scheduler --reset-meters --reset-optimizer \
--log-format simple --max-update=1000000 \
--share-decoder-input-output-embed \
--optimizer adam --adam-betas '(0.9, 0.99)' \
--warmup-init-lr 1e-07 --lr 2e-4 \
--lr-scheduler inverse_sqrt --warmup-updates 1000 \
--weight-decay 0.001 --criterion label_smoothed_cross_entropy \
--label-smoothing 0.5 --ddp-backend legacy_ddp --clip-norm 1 \
--patience 500 --batch-size 4096 --max-tokens 4096 \
--no-epoch-checkpoints --skip-invalid-size-inputs-valid-test \
--wandb-project $model_folder_name\_stage2_$model_desc_folder_name\_$epoch_name \
--fp16 --save-dir $MODEL_STAGE2/$model_folder_name/$model_desc/$date_format \
--log-file log_custom_fldr/$model_folder_name/$model_desc/train_stage2_$date_format.log \
>> log_custom_fldr/$model_folder_name/$model_desc/train_execution_stage2_$date_format.log 2>&1 &

bash generate_output.sh -n $model_folder_name -m $BERT_MODEL -u $CODE -k translation_with_pretrained_encoder_model -d $model_desc -s $source_lang -t $target_lang -a 3 -f $date_format -c best -b stage1

###############################################################
###############################################################
###############################################################
###############################################################3
##### for verifying the f.multi_head_attention_forward
##### COPE applied at the self attention component of decoder as well
## eng_lmn

source_lang=eng
target_lang=lmn
date_format=2024_06_09_19_45
model_desc_folder_name=cope_en_sattn
epoch_name=epoch_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

source_lang=eng
target_lang=lmn
older_date_format=2024_06_09_19_45
date_format=2024_06_09_20_45
model_desc_folder_name=cope_en_sattn
epoch_name=epoch_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

source_lang=eng
target_lang=lmn
older_date_format=2024_06_09_19_45
date_format=2024_06_09_20_47
model_desc_folder_name=cope_en_sattn
epoch_name=epoch_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

## eng_hin_10K

source_lang=eng_10K
target_lang=hin_10K
date_format=2024_06_09_21_45
model_desc_folder_name=cope_en_sattn
epoch_name=epoch_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_hin_10K

source_lang=eng_10K
target_lang=hin_10K
older_date_format=2024_06_09_21_45
date_format=2024_06_09_22_45
model_desc_folder_name=cope_en_sattn
epoch_name=epoch_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_hin_10K

source_lang=eng_10K
target_lang=hin_10K
older_date_format=2024_06_09_21_45
date_format=2024_06_09_22_47
model_desc_folder_name=cope_en_sattn
epoch_name=epoch_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_hin_10K

###############################################################
###############################################################
###############################################################
###############################################################
##### COPE applied at the self attention component only
## eng_lmn

source_lang=eng
target_lang=lmn
date_format=2024_06_10_14_01
model_desc_folder_name=cope_sattn
epoch_name=epoch_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

source_lang=eng
target_lang=lmn
older_date_format=2024_06_10_14_01
date_format=2024_06_10_15_01
model_desc_folder_name=cope_sattn
epoch_name=epoch_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

source_lang=eng
target_lang=lmn
older_date_format=2024_06_10_14_01
date_format=2024_06_10_15_03
model_desc_folder_name=cope_sattn
epoch_name=epoch_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

## eng_hin_10K

source_lang=eng_10K
target_lang=hin_10K
date_format=2024_06_10_16_01
model_desc_folder_name=cope_sattn
epoch_name=epoch_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_hin_10K

source_lang=eng_10K
target_lang=hin_10K
older_date_format=2024_06_10_16_01
date_format=2024_06_10_17_01
model_desc_folder_name=cope_sattn
epoch_name=epoch_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_hin_10K

source_lang=eng_10K
target_lang=hin_10K
older_date_format=2024_06_10_16_01
date_format=2024_06_10_17_03
model_desc_folder_name=cope_sattn
epoch_name=epoch_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_hin_10K


CUDA_VISIBLE_DEVICES=0 fairseq-train \
$DATA/$model_folder_name/$model_desc \
-s $source_lang \
-t $target_lang \
--user-dir $CODE \
--task translation_with_pretrained_encoder_model \
--model $BERT_MODEL \
--model-name bert \
--arch transformer_with_pretrained_bert_d5_emb512 \
--fine-tuning \
--restore-file /home1/ai21mtech12003/recycle_bert/model.stage2/$model_folder_name\/$model_desc\/$date_format\/bkp/checkpoint_last.pt \
--log-format simple --max-update=1000000 \
--share-decoder-input-output-embed \
--optimizer adam --adam-betas '(0.9, 0.99)' \
--warmup-init-lr 1e-07 --lr 2e-4 \
--lr-scheduler inverse_sqrt --warmup-updates 1000 \
--weight-decay 0.001 --criterion label_smoothed_cross_entropy \
--label-smoothing 0.5 --ddp-backend legacy_ddp --clip-norm 1 \
--max-epoch 1 --batch-size 1 \
--no-epoch-checkpoints --skip-invalid-size-inputs-valid-test \
--fp16 --save-dir $MODEL_STAGE2/$model_folder_name/$model_desc/$date_format