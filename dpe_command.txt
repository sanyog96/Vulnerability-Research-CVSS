##### dpe configuration for getting best tokens
source_lang=eng
target_lang=unr
date_format=2024_02_04_22_58
model_desc_folder_name=dpe_configuration_spm_s2500_t2500
epoch_name=epoch_50_enc6_e768_f768_h4_dec6_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=2-wayData/english_mundari

TEXT=examples/translation/2-wayData/english_mundari/spm_2500
fairseq-preprocess --source-lang $source_lang --target-lang $target_lang \
--trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \
--destdir data-bin/$model_folder_name/$model_desc --workers 20 \
--joined-dictionary \
>> log_custom_fldr/$model_folder_name/$model_desc/preprocess_$date_format.log

CUDA_VISIBLE_DEVICES=2 \
nohup fairseq-train data-bin/$model_folder_name/$model_desc \
--max-source-positions=210 --max-target-positions=210 --segment --raw-text \
--save-interval=1 --arch transformer_iwslt_en_hi --task translation \
-s $source_lang -t $target_lang --optimizer adam --lr 0.0005 \
--max-tokens 2048 --clip-norm 0.0 \
--min-lr '1e-09' --lr-scheduler inverse_sqrt --weight-decay 0.0 \
--criterion dynamic_programming_cross_entropy --max-epoch 50 --update-freq 1 \
--warmup-updates 4000 --warmup-init-lr '1e-07' \
--adam-betas '(0.9, 0.98)' --save-dir checkpoints/$model_folder_name/$model_desc/$date_format \
--skip-invalid-size-inputs-valid-test \
--share-all-embeddings --no-epoch-checkpoints --seed 124 >> log_custom_fldr/$model_folder_name/$model_desc/train_execution_$date_format.log 2>&1 &

python3 ./find_seg.py data-bin/$model_folder_name/$model_desc \
--task translation --segment --raw-text -s $source_lang -t $target_lang \
--path checkpoints/$model_folder_name/$model_desc/$date_format/checkpoint_best.pt \
--batch-size 40 --max-len-b 210 --gen-subset test --skip-invalid-size-inputs-valid-test \
> data-bin/$model_folder_name/$model_desc/test_dpe.unr

python3 ./find_seg.py data-bin/$model_folder_name/$model_desc \
--task translation --segment --raw-text -s $source_lang -t $target_lang \
--path checkpoints/$model_folder_name/$model_desc/$date_format/checkpoint_best.pt \
--batch-size 40 --max-len-b 210 --gen-subset valid --skip-invalid-size-inputs-valid-test \
> data-bin/$model_folder_name/$model_desc/valid_dpe.unr

python3 ./find_seg.py data-bin/$model_folder_name/$model_desc \
--task translation --segment --raw-text -s $source_lang -t $target_lang \
--path checkpoints/$model_folder_name/$model_desc/$date_format/checkpoint_best.pt \
--batch-size 40 --max-len-b 210 --gen-subset train --skip-invalid-size-inputs-valid-test \
> data-bin/$model_folder_name/$model_desc/train_dpe.unr

sort -n data-bin/$model_folder_name/$model_desc/train_dpe.unr > data-bin/$model_folder_name/$model_desc/train_dpe.unr_sorted
sort -n data-bin/$model_folder_name/$model_desc/valid_dpe.unr > data-bin/$model_folder_name/$model_desc/valid_dpe.unr_sorted
sort -n data-bin/$model_folder_name/$model_desc/test_dpe.unr > data-bin/$model_folder_name/$model_desc/test_dpe.unr_sorted

python3 postprocess_dpe.py data-bin/$model_folder_name/$model_desc train eng unr
python3 postprocess_dpe.py data-bin/$model_folder_name/$model_desc test eng unr
python3 postprocess_dpe.py data-bin/$model_folder_name/$model_desc valid eng unr

export BERT_MODEL=./uncased_L-12_H-768_A-12
export CODE=./user_code
export CORPUS=./corpus
export DATA=./data
export MODEL_STAGE1=./model.stage1
export MODEL_STAGE2=./model.stage2
export PYTHONPATH="$CODE:$PYTHONPATH"
source_lang=eng
target_lang=unr
date_format=2024_02_05_11_14
model_desc_folder_name=dpe_conf_spmt_2500
epoch_name=pat_500_enc12_e768_f768_h12_dec6_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_unr

python3 $CODE/preprocess.py --source-lang $source_lang --target-lang $target_lang \
--srcdict $DATA/$model_folder_name/$model_desc/dict.eng.txt \
--trainpref ../dpe_dataset/$model_folder_name/output/train \
--validpref ../dpe_dataset/$model_folder_name/output/valid \
--testpref ../dpe_dataset/$model_folder_name/output/test \
--destdir $DATA/$model_folder_name/$model_desc \
--log-file log_custom_fldr/$model_folder_name/$model_desc/$date_format/preprocess_$date_format.log

CUDA_VISIBLE_DEVICES=7 \
nohup fairseq-train \
$DATA/$model_folder_name/$model_desc \
-s $source_lang \
-t $target_lang \
--user-dir $CODE \
--task translation_with_pretrained_encoder_model \
--model $BERT_MODEL \
--model-name bert \
--arch transformer_with_pretrained_bert_d5_emb512 \
--log-format simple --max-update=1000000 \
--share-decoder-input-output-embed \
--optimizer adam --adam-betas '(0.9, 0.99)' \
--warmup-init-lr 1e-07 --lr 5e-4 \
--lr-scheduler inverse_sqrt --warmup-updates 4000 \
--weight-decay 0.001 --criterion label_smoothed_cross_entropy \
--label-smoothing 0.5 --ddp-backend legacy_ddp --clip-norm 1.0 \
--patience 500 --batch-size 4096 --max-tokens 4096 --skip-invalid-size-inputs-valid-test \
--wandb-project $model_folder_name\_stage1_$model_desc_folder_name\_$epoch_name \
--fp16 --save-dir $MODEL_STAGE1/$model_folder_name/$model_desc/$date_format \
--log-file log_custom_fldr/$model_folder_name/$model_desc/train_$date_format.log \
>> log_custom_fldr/$model_folder_name/$model_desc/train_execution_$date_format.log 2>&1 &

bash stage1_generate_output.sh -n $model_folder_name -m $BERT_MODEL \
-u $CODE -k translation_with_bert -d $model_desc -s $source_lang \
-t $target_lang -a 2 -f $date_format

source_lang=eng
target_lang=unr
date_format=2024_02_05_14_11
model_desc_folder_name=dpe_conf_spmt_2500
epoch_name=pat_500_enc12_e768_f768_h12_dec6_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_unr

CUDA_VISIBLE_DEVICES=6,7 \
nohup fairseq-train \
$DATA/$model_folder_name/$model_desc \
-s $source_lang \
-t $target_lang \
--user-dir $CODE \
--task translation_with_pretrained_encoder_model \
--model $BERT_MODEL \
--model-name bert \
--arch transformer_with_pretrained_bert_d5_emb512 \
--fine-tuning \
--restore-file /home1/ai21mtech12003/recycle_bert/model.stage1/$model_folder_name\/$model_desc\/$older_date_format\/checkpoint_best.pt \
--reset-lr-scheduler --reset-meters --reset-optimizer \
--log-format simple --max-update=1000000 \
--share-decoder-input-output-embed \
--optimizer adam --adam-betas '(0.9, 0.99)' \
--warmup-init-lr 1e-07 --lr 2e-4 \
--lr-scheduler inverse_sqrt --warmup-updates 1000 \
--weight-decay 0.001 --criterion label_smoothed_cross_entropy \
--label-smoothing 0.5 --ddp-backend legacy_ddp --clip-norm 1 \
--patience 500 --batch-size 4096 --max-tokens 4096 --no-epoch-checkpoints --skip-invalid-size-inputs-valid-test \
--wandb-project $model_folder_name\_stage2_$model_desc_folder_name\_$epoch_name \
--fp16 --save-dir $MODEL_STAGE2/$model_folder_name/$model_desc/$date_format \
--log-file log_custom_fldr/$model_folder_name/$model_desc/train_stage2_$date_format.log \
>> log_custom_fldr/$model_folder_name/$model_desc/train_execution_stage2_$date_format.log 2>&1 &

bash generate_output.sh -n $model_folder_name -m $BERT_MODEL -u $CODE -k translation_with_pretrained_encoder_model -d $model_desc -s $source_lang -t $target_lang -a 6 -f $date_format -c best -b stage1





### Eng-lmn
source_lang=eng
target_lang=lmn
date_format=2024_05_10_15_01
model_desc_folder_name=dpe_configuration_spm_s2500_t2500
epoch_name=epoch_50_enc6_e768_f3072_h4_dec6_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=2-wayData/english_lambani

export BERT_MODEL=./uncased_L-12_H-768_A-12
export CODE=./user_code
export CORPUS=./corpus
export DATA=./data
export MODEL_STAGE1=./model.stage1
export MODEL_STAGE2=./model.stage2
export PYTHONPATH="$CODE:$PYTHONPATH"
source_lang=eng
target_lang=lmn
date_format=2024_05_22_22_11
model_desc_folder_name=dpe_conf_spmt_2500
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

python3 seg_data.py /home1/ai21mtech12003/bkp_fairseq/$model_folder_name\/spm_2500 $source_lang\-$target_lang

TEXT=../bkp_fairseq/$model_folder_name/spm_2500/output
fairseq-preprocess --source-lang $source_lang --target-lang $target_lang \
--trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \
--destdir data-bin/$model_folder_name/$model_desc --workers 20 \
--joined-dictionary \
>> log_custom_fldr/$model_folder_name/$model_desc/preprocess_$date_format.log

python3 build_dict.py data-bin/$model_folder_name\/$model_desc $source_lang\-$target_lang \
> data-bin/$model_folder_name\/$model_desc\/dict.$target_lang\.in.txt

CUDA_VISIBLE_DEVICES=3 \
nohup fairseq-train data-bin/$model_folder_name/$model_desc \
--max-source-positions=210 --max-target-positions=210 --segment --raw-text \
--save-interval=1 --arch transformer_iwslt_en_hi --task translation \
-s $source_lang -t $target_lang --optimizer adam --lr 0.0005 \
--max-tokens 2048 --clip-norm 0.5 \
--min-lr '1e-09' --lr-scheduler inverse_sqrt --weight-decay 0.0 \
--criterion dynamic_programming_cross_entropy --max-epoch 100 --update-freq 1 \
--warmup-updates 4000 --warmup-init-lr '1e-07' \
--adam-betas '(0.9, 0.98)' --save-dir checkpoints/$model_folder_name/$model_desc/$date_format \
--skip-invalid-size-inputs-valid-test \
--share-all-embeddings --no-epoch-checkpoints --seed 124 \
>> log_custom_fldr/$model_folder_name/$model_desc/train_execution_$date_format.log 2>&1 &

CUDA_VISIBLE_DEVICES=4 python3 ./find_seg.py data-bin/$model_folder_name/$model_desc \
--task translation --segment --raw-text -s $source_lang -t $target_lang \
--path checkpoints/$model_folder_name/$model_desc/$date_format/checkpoint_best.pt \
--batch-size 40 --max-len-b 100 --gen-subset test --skip-invalid-size-inputs-valid-test \
> data-bin/$model_folder_name/$model_desc/test_dpe.$target_lang

CUDA_VISIBLE_DEVICES=5 python3 ./find_seg.py data-bin/$model_folder_name/$model_desc \
--task translation --segment --raw-text -s $source_lang -t $target_lang \
--path checkpoints/$model_folder_name/$model_desc/$date_format/checkpoint_best.pt \
--batch-size 40 --max-len-b 210 --gen-subset valid --skip-invalid-size-inputs-valid-test \
> data-bin/$model_folder_name/$model_desc/valid_dpe.$target_lang

CUDA_VISIBLE_DEVICES=4 python3 ./find_seg.py data-bin/$model_folder_name/$model_desc \
--task translation --segment --raw-text -s $source_lang -t $target_lang \
--path checkpoints/$model_folder_name/$model_desc/$date_format/checkpoint_best.pt \
--batch-size 100 --max-len-b 210 --gen-subset train --skip-invalid-size-inputs-valid-test \
> data-bin/$model_folder_name/$model_desc/train_dpe.$target_lang

sort -n data-bin/$model_folder_name/$model_desc/train_dpe.$target_lang > data-bin/$model_folder_name/$model_desc/train_dpe.$target_lang\_sorted
sort -n data-bin/$model_folder_name/$model_desc/valid_dpe.$target_lang > data-bin/$model_folder_name/$model_desc/valid_dpe.$target_lang\_sorted
sort -n data-bin/$model_folder_name/$model_desc/test_dpe.$target_lang > data-bin/$model_folder_name/$model_desc/test_dpe.$target_lang\_sorted

python3 postprocess_dpe.py data-bin/$model_folder_name/$model_desc train $source_lang $target_lang
python3 postprocess_dpe.py data-bin/$model_folder_name/$model_desc test $source_lang $target_lang
python3 postprocess_dpe.py data-bin/$model_folder_name/$model_desc valid $source_lang $target_lang

## eng-lmn
source_lang=eng
target_lang=lmn
date_format=2024_05_10_15_01
model_desc_folder_name=dpe_configuration_spm_s2500_t2500
epoch_name=epoch_50_enc6_e768_f3072_h4_dec6_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=2-wayData/english_lambani

export BERT_MODEL=./uncased_L-12_H-768_A-12
export CODE=./user_code
export CORPUS=./corpus
export DATA=./data
export MODEL_STAGE1=./model.stage1
export MODEL_STAGE2=./model.stage2
export PYTHONPATH="$CODE:$PYTHONPATH"

source_lang=eng
target_lang=lmn
date_format=2024_05_22_22_11
model_desc_folder_name=dpe_conf_spmt_2500
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

source_lang=eng
target_lang=lmn
older_date_format=2024_05_22_22_11
date_format=2024_05_23_14_41
model_desc_folder_name=dpe_conf_spmt_2500
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

### Eng-sle
source_lang=eng
target_lang=sle
date_format=2024_05_10_15_42
model_desc_folder_name=dpe_configuration_spm_s2500_t2500
epoch_name=epoch_50_enc6_e768_f3072_h4_dec6_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=2-wayData/english_soliga

source_lang=eng
target_lang=sle
date_format=2024_05_22_22_32
model_desc_folder_name=dpe_conf_spmt_2500
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_sle

source_lang=eng
target_lang=sle
older_date_format=2024_05_22_22_32
date_format=2024_05_22_23_19
model_desc_folder_name=dpe_conf_spmt_2500
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_sle

### Eng-uki
source_lang=eng
target_lang=uki
date_format=2024_05_10_15_50
model_desc_folder_name=dpe_configuration_spm_s2500_t2500
epoch_name=epoch_50_enc6_e768_f3072_h4_dec6_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=2-wayData/english_kui

source_lang=eng
target_lang=uki
date_format=2024_05_22_22_38
model_desc_folder_name=dpe_conf_spmt_2500
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_uki

source_lang=eng
target_lang=uki
older_date_format=2024_05_22_22_38
date_format=2024_05_23_18_11
model_desc_folder_name=dpe_conf_spmt_2500
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_uki

### Eng-unr
source_lang=eng
target_lang=unr
date_format=2024_05_22_11_38
model_desc_folder_name=dpe_configuration_spm_s2500_t2500
epoch_name=epoch_50_enc6_e768_f3072_h4_dec6_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=2-wayData/english_mundari

source_lang=eng
target_lang=unr
date_format=2024_05_22_22_41
model_desc_folder_name=dpe_conf_spmt_2500
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_unr

source_lang=eng
target_lang=unr
older_date_format=2024_05_22_22_41
date_format=2024_05_22_14_25
model_desc_folder_name=dpe_conf_spmt_2500
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_unr

### eng_10K-hin_10K
source_lang=eng_10K
target_lang=hin_10K
date_format=2024_05_22_12_06
model_desc_folder_name=dpe_configuration_spm_s2500_t2500
epoch_name=epoch_50_enc6_e768_f3072_h4_dec6_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=2-wayData/eng_hin_10K

source_lang=eng_10K
target_lang=hin_10K
date_format=2024_05_22_22_45
model_desc_folder_name=dpe_conf_spmt_2500
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_hin_10K

source_lang=eng_10K
target_lang=hin_10K
older_date_format=2024_05_22_22_45
date_format=2024_05_23_18_14
model_desc_folder_name=dpe_conf_spmt_2500
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_hin_10K


## delete files recursively keeping the subdirectories intact
find . ! -name '.*' ! -type d -exec rm -- {} +