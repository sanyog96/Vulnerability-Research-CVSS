python create_pretraining_data.py \
  --input_file=/home/sanyog/bert/bert_dataset/lmn/train.lmn \
  --output_file=/home/sanyog/bert/bert_dataset/lmn/train_output.lmn \
  --vocab_file=/home/sanyog/bert/bert_dataset/lmn/vocab.txt \
  --max_seq_length=40 \
  --max_predictions_per_seq=10 \
  --masked_lm_prob=0.25 \
  --random_seed=12345 \
  --dupe_factor=5
 
python word_piece_trainer.py --tokenizer_name bert-base-uncased \
--data_dir /home/sanyog/bert/bert_dataset/uki \
--batch_size 500 --vocab_size 2500 --save_fp tokenizer/uki

python word_piece_trainer.py --tokenizer_name bert-base-uncased \
--data_dir /home/sanyog/bert/bert_dataset/unr \
--batch_size 500 --vocab_size 2500 --save_fp tokenizer/unr

https://github.com/AliHaiderAhmad001/BERT-from-Scratch-with-PyTorch

CUDA_VISIBLE_DEVICES=0 python3 main.py /home/sanyog/BERT-from-Scratch-with-PyTorch/tokenizer/lmn

python3 $CODE/preprocess.py --source-lang $source_lang --target-lang $target_lang \
--srcdict $DATA/$model_folder_name/$model_desc/dict.eng_10K.txt \
--trainpref ../deltalm_dataset/$model_folder_name/train \
--validpref ../deltalm_dataset/$model_folder_name/valid \
--testpref ../deltalm_dataset/$model_folder_name/test \
--destdir $DATA/$model_folder_name/$model_desc \
--log-file log_custom_fldr/$model_folder_name/$model_desc/$date_format/preprocess_$date_format.log

CUDA_VISIBLE_DEVICES=1 \
nohup fairseq-train \
$DATA/$model_folder_name/$model_desc \
-s $source_lang \
-t $target_lang \
--task translation_with_pretrained_model \
--pretrained-deltalm-checkpoint $PRETRAINED_MODEL \
--user-dir $CODE \
--encoder-model $ENCODER_MODEL \
--decoder-model $DECODER_MODEL \
--arch deltalm_base_512 \
--log-format simple --max-update=1000000 \
--share-decoder-input-output-embed \
--optimizer adam --adam-betas '(0.9, 0.99)' \
--warmup-init-lr 1e-07 --lr 5e-4 \
--lr-scheduler inverse_sqrt --warmup-updates 4000 \
--weight-decay 0.001 --criterion label_smoothed_cross_entropy \
--label-smoothing 0.5 --ddp-backend legacy_ddp --clip-norm 1.0 \
--patience 500 --batch-size 4096 --max-tokens 4096 --no-epoch-checkpoints \
--skip-invalid-size-inputs-valid-test \
--wandb-project $model_folder_name\_stage1_$model_desc_folder_name\_$epoch_name \
--fp16 --save-dir $MODEL_STAGE1/$model_folder_name/$model_desc/$date_format \
--log-file log_custom_fldr/$model_folder_name/$model_desc/train_$date_format.log \
>> log_custom_fldr/$model_folder_name/$model_desc/train_execution_$date_format.log 2>&1 &

bash generate_output.sh -n $model_folder_name -m $ENCODER_MODEL -q $DECODER_MODEL -u $CODE -k translation_with_pretrained_model -d $model_desc -s $source_lang -t $target_lang -a 6 -f $date_format -c best -b stage1

CUDA_VISIBLE_DEVICES=6 \
nohup fairseq-train \
$DATA/$model_folder_name/$model_desc \
-s $source_lang \
-t $target_lang \
--task translation_with_pretrained_model \
--pretrained-deltalm-checkpoint $PRETRAINED_MODEL \
--user-dir $CODE \
--encoder-model $ENCODER_MODEL \
--decoder-model $DECODER_MODEL \
--arch deltalm_base_512 \
--encoder-fine-tuning \
--restore-file /home1/ai21mtech12003/encoder_decoder/model.stage1/$model_folder_name\/$model_desc\/$older_date_format\/checkpoint_best.pt \
--reset-lr-scheduler --reset-meters --reset-optimizer \
--log-format simple --max-update=1000000 \
--share-decoder-input-output-embed \
--optimizer adam --adam-betas '(0.9, 0.99)' \
--warmup-init-lr 1e-07 --lr 2e-4 \
--lr-scheduler inverse_sqrt --warmup-updates 1000 \
--weight-decay 0.001 --criterion label_smoothed_cross_entropy \
--label-smoothing 0.5 --ddp-backend legacy_ddp --clip-norm 1 \
--patience 500 --batch-size 4096 --max-tokens 4096 --no-epoch-checkpoints \
--skip-invalid-size-inputs-valid-test \
--wandb-project $model_folder_name\_stage2_$model_desc_folder_name\_$epoch_name \
--fp16 --save-dir $MODEL_STAGE2/$model_folder_name/$model_desc/$date_format \
--log-file log_custom_fldr/$model_folder_name/$model_desc/train_stage2_$date_format.log \
>> log_custom_fldr/$model_folder_name/$model_desc/train_execution_stage2_$date_format.log 2>&1 &

#### eng_lmn

export ENCODER_MODEL=/home1/ai21mtech12003/recycle_bert/uncased_L-12_H-768_A-12
export DECODER_MODEL=./lmn-bert
PRETRAINED_MODEL=./lmn-bert/pytorch_model.bin
export CODE=./user_code
export CORPUS=./corpus
export DATA=./data
export MODEL_STAGE1=./model.stage1
export MODEL_STAGE2=./model.stage2
export PYTHONPATH="$CODE:$PYTHONPATH"

source_lang=eng
target_lang=lmn
date_format=2024_06_01_13_35
model_desc_folder_name=deltaLM_encdecbert_t2500
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

#best checkpoint from stage1
source_lang=eng
target_lang=lmn
older_date_format=2024_06_01_13_35
date_format=2024_06_01_14_35
model_desc_folder_name=deltaLM_encdecbert_t2500
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

#last checkpoint from stage1
source_lang=eng
target_lang=lmn
older_date_format=2024_06_01_13_35
date_format=2024_06_01_14_37
model_desc_folder_name=deltaLM_encdecbert_t2500
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

### eng_uki

export ENCODER_MODEL=/home1/ai21mtech12003/recycle_bert/uncased_L-12_H-768_A-12
export DECODER_MODEL=./uki-bert
PRETRAINED_MODEL=./uki-bert/pytorch_model.bin
export CODE=./user_code
export CORPUS=./corpus
export DATA=./data
export MODEL_STAGE1=./model.stage1
export MODEL_STAGE2=./model.stage2
export PYTHONPATH="$CODE:$PYTHONPATH"

source_lang=eng
target_lang=uki
date_format=2024_06_01_14_16
model_desc_folder_name=deltaLM_encdecbert_t2500
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_uki

##best checkpoint from stage1
source_lang=eng
target_lang=uki
older_date_format=2024_06_01_14_16
date_format=2024_06_01_15_16
model_desc_folder_name=deltaLM_encdecbert_t2500
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_uki

##last checkpoint from stage1
source_lang=eng
target_lang=uki
older_date_format=2024_06_01_14_16
date_format=2024_06_01_15_17
model_desc_folder_name=deltaLM_encdecbert_t2500
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_uki

### eng_unr

export ENCODER_MODEL=/home1/ai21mtech12003/recycle_bert/uncased_L-12_H-768_A-12
export DECODER_MODEL=./unr-bert
PRETRAINED_MODEL=./unr-bert/pytorch_model.bin
export CODE=./user_code
export CORPUS=./corpus
export DATA=./data
export MODEL_STAGE1=./model.stage1
export MODEL_STAGE2=./model.stage2
export PYTHONPATH="$CODE:$PYTHONPATH"

source_lang=eng
target_lang=unr
date_format=2024_06_01_14_21
model_desc_folder_name=deltaLM_encdecbert_t2500
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_unr

##best checkpoint from stage1
source_lang=eng
target_lang=unr
older_date_format=2024_06_01_14_21
date_format=2024_06_01_15_21
model_desc_folder_name=deltaLM_encdecbert_t2500
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_unr

##last checkpoint from stage1
source_lang=eng
target_lang=unr
older_date_format=2024_06_01_14_21
date_format=2024_06_01_15_22
model_desc_folder_name=deltaLM_encdecbert_t2500
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_unr

### eng_hin_10K

export ENCODER_MODEL=/home1/ai21mtech12003/recycle_bert/uncased_L-12_H-768_A-12
export DECODER_MODEL=./hindi-bert
PRETRAINED_MODEL=./hindi-bert/pytorch_model.bin
export CODE=./user_code
export CORPUS=./corpus
export DATA=./data
export MODEL_STAGE1=./model.stage1
export MODEL_STAGE2=./model.stage2
export PYTHONPATH="$CODE:$PYTHONPATH"

source_lang=eng_10K
target_lang=hin_10K
date_format=2024_06_01_14_26
model_desc_folder_name=deltaLM_encdecbert_t2500
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_hin_10K

##best checkpoint from stage1
source_lang=eng_10K
target_lang=hin_10K
older_date_format=2024_06_01_14_26
date_format=2024_06_01_15_26
model_desc_folder_name=deltaLM_encdecbert_t2500
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_hin_10K

##last checkpoint from stage1
source_lang=eng_10K
target_lang=hin_10K
older_date_format=2024_06_01_14_26
date_format=2024_06_01_15_28
model_desc_folder_name=deltaLM_encdecbert_t2500
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_hin_10K



######################################################################################
######################################################################################
######################################################################################
######################################################################################
Target BERT Dictionary used for tokenization

#### eng_lmn

cat ../bkp_fairseq/2-wayData/english_lambani/tmp/train.lmn | python3 $CODE/bert_tokenize.py --model=$DECODER_MODEL > ../deltalm_dataset/eng_lmn/t_bert_dict/train.lmn
cat ../bkp_fairseq/2-wayData/english_lambani/tmp/test.lmn | python3 $CODE/bert_tokenize.py --model=$DECODER_MODEL > ../deltalm_dataset/eng_lmn/t_bert_dict/test.lmn
cat ../bkp_fairseq/2-wayData/english_lambani/tmp/valid.lmn | python3 $CODE/bert_tokenize.py --model=$DECODER_MODEL > ../deltalm_dataset/eng_lmn/t_bert_dict/valid.lmn

export ENCODER_MODEL=/home1/ai21mtech12003/recycle_bert/uncased_L-12_H-768_A-12
export DECODER_MODEL=./lmn-bert
PRETRAINED_MODEL=./lmn-bert/pytorch_model.bin
export CODE=./user_code
export CORPUS=./corpus
export DATA=./data
export MODEL_STAGE1=./model.stage1
export MODEL_STAGE2=./model.stage2
export PYTHONPATH="$CODE:$PYTHONPATH"

source_lang=eng
target_lang=lmn
date_format=2024_06_02_10_02
model_desc_folder_name=deltaLM_encdecbert_tbertdict
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

#best checkpoint from stage1
source_lang=eng
target_lang=lmn
older_date_format=2024_06_02_10_02
date_format=2024_06_02_11_02
model_desc_folder_name=deltaLM_encdecbert_tbertdict
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

#last checkpoint from stage1
source_lang=eng
target_lang=lmn
older_date_format=2024_06_02_10_02
date_format=2024_06_02_11_04
model_desc_folder_name=deltaLM_encdecbert_tbertdict
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

#### eng_uki

cat ../bkp_fairseq/2-wayData/english_kui/tmp/train.uki | python3 $CODE/bert_tokenize.py --model=$DECODER_MODEL > ../deltalm_dataset/eng_uki/t_bert_dict/train.uki
cat ../bkp_fairseq/2-wayData/english_kui/tmp/test.uki | python3 $CODE/bert_tokenize.py --model=$DECODER_MODEL > ../deltalm_dataset/eng_uki/t_bert_dict/test.uki
cat ../bkp_fairseq/2-wayData/english_kui/tmp/valid.uki | python3 $CODE/bert_tokenize.py --model=$DECODER_MODEL > ../deltalm_dataset/eng_uki/t_bert_dict/valid.uki

export ENCODER_MODEL=/home1/ai21mtech12003/recycle_bert/uncased_L-12_H-768_A-12
export DECODER_MODEL=./uki-bert
PRETRAINED_MODEL=./uki-bert/pytorch_model.bin
export CODE=./user_code
export CORPUS=./corpus
export DATA=./data
export MODEL_STAGE1=./model.stage1
export MODEL_STAGE2=./model.stage2
export PYTHONPATH="$CODE:$PYTHONPATH"

source_lang=eng
target_lang=uki
date_format=2024_06_02_10_05
model_desc_folder_name=deltaLM_encdecbert_tbertdict
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_uki

##best checkpoint from stage1
source_lang=eng
target_lang=uki
older_date_format=2024_06_02_10_05
date_format=2024_06_02_11_05
model_desc_folder_name=deltaLM_encdecbert_tbertdict
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_uki

##last checkpoint from stage1
source_lang=eng
target_lang=uki
older_date_format=2024_06_02_10_05
date_format=2024_06_02_11_07
model_desc_folder_name=deltaLM_encdecbert_tbertdict
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_uki

##### eng_unr

cat ../bkp_fairseq/2-wayData/english_mundari/tmp/train.unr | python3 $CODE/bert_tokenize.py --model=$DECODER_MODEL > ../deltalm_dataset/eng_unr/t_bert_dict/train.unr
cat ../bkp_fairseq/2-wayData/english_mundari/tmp/test.unr | python3 $CODE/bert_tokenize.py --model=$DECODER_MODEL > ../deltalm_dataset/eng_unr/t_bert_dict/test.unr
cat ../bkp_fairseq/2-wayData/english_mundari/tmp/valid.unr | python3 $CODE/bert_tokenize.py --model=$DECODER_MODEL > ../deltalm_dataset/eng_unr/t_bert_dict/valid.unr

export ENCODER_MODEL=/home1/ai21mtech12003/recycle_bert/uncased_L-12_H-768_A-12
export DECODER_MODEL=./unr-bert
PRETRAINED_MODEL=./unr-bert/pytorch_model.bin
export CODE=./user_code
export CORPUS=./corpus
export DATA=./data
export MODEL_STAGE1=./model.stage1
export MODEL_STAGE2=./model.stage2
export PYTHONPATH="$CODE:$PYTHONPATH"

source_lang=eng
target_lang=unr
date_format=2024_06_02_10_10
model_desc_folder_name=deltaLM_encdecbert_tbertdict
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_unr

##best checkpoint from stage1
source_lang=eng
target_lang=unr
older_date_format=2024_06_02_10_10
date_format=2024_06_02_11_10
model_desc_folder_name=deltaLM_encdecbert_tbertdict
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_unr

##last checkpoint from stage1
source_lang=eng
target_lang=unr
older_date_format=2024_06_02_10_10
date_format=2024_06_02_11_12
model_desc_folder_name=deltaLM_encdecbert_tbertdict
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_unr

##### eng_hin_10K

cat ../bkp_fairseq/2-wayData/eng_hin_10K/tmp/train.hin_10K | python3 $CODE/bert_tokenize.py --model=$DECODER_MODEL > ../deltalm_dataset/eng_hin_10K/t_bert_dict/train.hin_10K
cat ../bkp_fairseq/2-wayData/eng_hin_10K/tmp/test.hin_10K | python3 $CODE/bert_tokenize.py --model=$DECODER_MODEL > ../deltalm_dataset/eng_hin_10K/t_bert_dict/test.hin_10K
cat ../bkp_fairseq/2-wayData/eng_hin_10K/tmp/valid.hin_10K | python3 $CODE/bert_tokenize.py --model=$DECODER_MODEL > ../deltalm_dataset/eng_hin_10K/t_bert_dict/valid.hin_10K

export ENCODER_MODEL=/home1/ai21mtech12003/recycle_bert/uncased_L-12_H-768_A-12
export DECODER_MODEL=./hindi-bert
PRETRAINED_MODEL=./hindi-bert/pytorch_model.bin
export CODE=./user_code
export CORPUS=./corpus
export DATA=./data
export MODEL_STAGE1=./model.stage1
export MODEL_STAGE2=./model.stage2
export PYTHONPATH="$CODE:$PYTHONPATH"

source_lang=eng_10K
target_lang=hin_10K
date_format=2024_06_02_10_15
model_desc_folder_name=deltaLM_encdecbert_tbertdict
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_hin_10K

##best checkpoint from stage1
source_lang=eng_10K
target_lang=hin_10K
older_date_format=2024_06_02_10_15
date_format=2024_06_02_11_15
model_desc_folder_name=deltaLM_encdecbert_tbertdict
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_hin_10K

##last checkpoint from stage1
source_lang=eng_10K
target_lang=hin_10K
older_date_format=2024_06_02_10_15
date_format=2024_06_02_11_17
model_desc_folder_name=deltaLM_encdecbert_tbertdict
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_hin_10K

cat $DECODER_MODEL/vocab.txt | tail -n +5 | \
sed -e 's/$/ 0/' > $DATA/$model_folder_name/$model_desc/dict.$target_lang\.txt
 
python3 $CODE/preprocess.py --source-lang $source_lang --target-lang $target_lang \
--srcdict $DATA/$model_folder_name/$model_desc/dict.$source_lang\.txt \
--tgtdict $DATA/$model_folder_name/$model_desc/dict.$target_lang\.txt \
--trainpref ../deltalm_dataset/$model_folder_name/t_bert_dict/train \
--validpref ../deltalm_dataset/$model_folder_name/t_bert_dict/valid \
--testpref ../deltalm_dataset/$model_folder_name/t_bert_dict/test \
--destdir $DATA/$model_folder_name/$model_desc \
--log-file log_custom_fldr/$model_folder_name/$model_desc/$date_format/preprocess_$date_format.log

CUDA_VISIBLE_DEVICES=4 \
nohup fairseq-train \
$DATA/$model_folder_name/$model_desc \
-s $source_lang \
-t $target_lang \
--task translation_with_pretrained_model \
--pretrained-deltalm-checkpoint $PRETRAINED_MODEL \
--user-dir $CODE \
--encoder-model $ENCODER_MODEL \
--decoder-model $DECODER_MODEL \
--arch deltalm_base_512 \
--log-format simple --max-update=1000000 \
--share-decoder-input-output-embed \
--optimizer adam --adam-betas '(0.9, 0.99)' \
--warmup-init-lr 1e-07 --lr 5e-4 \
--lr-scheduler inverse_sqrt --warmup-updates 4000 \
--weight-decay 0.001 --criterion label_smoothed_cross_entropy \
--label-smoothing 0.5 --ddp-backend legacy_ddp --clip-norm 1.0 \
--patience 500 --batch-size 4096 --max-tokens 4096 --no-epoch-checkpoints \
--skip-invalid-size-inputs-valid-test \
--wandb-project $model_folder_name\_stage1_$model_desc_folder_name\_$epoch_name \
--fp16 --save-dir $MODEL_STAGE1/$model_folder_name/$model_desc/$date_format \
--log-file log_custom_fldr/$model_folder_name/$model_desc/train_$date_format.log \
>> log_custom_fldr/$model_folder_name/$model_desc/train_execution_$date_format.log 2>&1 &

bash generate_output_bert.sh -n $model_folder_name -m $ENCODER_MODEL -q $DECODER_MODEL -u $CODE -k translation_with_pretrained_model -d $model_desc -s $source_lang -t $target_lang -a 6 -f $date_format -c best -b stage1

CUDA_VISIBLE_DEVICES=1 \
nohup fairseq-train \
$DATA/$model_folder_name/$model_desc \
-s $source_lang \
-t $target_lang \
--task translation_with_pretrained_model \
--pretrained-deltalm-checkpoint $PRETRAINED_MODEL \
--user-dir $CODE \
--encoder-model $ENCODER_MODEL \
--decoder-model $DECODER_MODEL \
--arch deltalm_base_512 \
--encoder-fine-tuning \
--restore-file /home1/ai21mtech12003/encoder_decoder/model.stage1/$model_folder_name\/$model_desc\/$older_date_format\/checkpoint_best.pt \
--reset-lr-scheduler --reset-meters --reset-optimizer \
--log-format simple --max-update=1000000 \
--share-decoder-input-output-embed \
--optimizer adam --adam-betas '(0.9, 0.99)' \
--warmup-init-lr 1e-07 --lr 2e-4 \
--lr-scheduler inverse_sqrt --warmup-updates 1000 \
--weight-decay 0.001 --criterion label_smoothed_cross_entropy \
--label-smoothing 0.5 --ddp-backend legacy_ddp --clip-norm 1 \
--patience 500 --batch-size 4096 --max-tokens 4096 --no-epoch-checkpoints \
--skip-invalid-size-inputs-valid-test \
--wandb-project $model_folder_name\_stage2_$model_desc_folder_name\_$epoch_name \
--fp16 --save-dir $MODEL_STAGE2/$model_folder_name/$model_desc/$date_format \
--log-file log_custom_fldr/$model_folder_name/$model_desc/train_stage2_$date_format.log \
>> log_custom_fldr/$model_folder_name/$model_desc/train_execution_stage2_$date_format.log 2>&1 &