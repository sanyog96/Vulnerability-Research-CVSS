Running tokenizer on dataset line_by_line (num_proc=4): 100%|█| 20884/20884
max_steps is given, it will override any value given in num_train_epochs
The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expecte
d by `BertForMaskedLM.forward`,  you can safely ignore this message.
/home/sanyog/anaconda3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use
the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 20884
  Num Epochs = 5
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 1500
  Number of trainable parameters = 26912708
{'loss': 6.9798, 'learning_rate': 2.8e-05, 'epoch': 0.31}
  7%|██▏                              | 100/1500 [51:53<11:58:57, 30.81s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and
have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 20884
  Batch size = 256
{'eval_loss': 6.661638259887695, 'eval_runtime': 2765.5213, 'eval_samples_per_second': 7.552, 'eval_steps_per_second': 0.03, 'epoch': 0.31}
{'loss': 6.5768, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.61}
 13%|████▏                          | 200/1500 [2:30:36<10:44:08, 29.73s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and
have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 20884
  Batch size = 256
                                                                            ^[[A^[[B                                     | 6/82 [02:54<40:37, 32.07s/it]
{'eval_loss': 6.455691814422607, 'eval_runtime': 2879.971, 'eval_samples_per_second': 7.251, 'eval_steps_per_second': 0.028, 'epoch': 0.61}
{'loss': 6.432, 'learning_rate': 2.4e-05, 'epoch': 0.92}
 20%|██████▏                        | 300/1500 [4:08:19<10:19:49, 30.99s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and
have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 20884
  Batch size = 256
{'eval_loss': 6.29120397567749, 'eval_runtime': 2855.1704, 'eval_samples_per_second': 7.314, 'eval_steps_per_second': 0.029, 'epoch': 0.92}
{'loss': 6.2507, 'learning_rate': 2.2e-05, 'epoch': 1.22}
 27%|████████▌                       | 400/1500 [5:42:36<8:52:40, 29.06s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and
have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 20884
  Batch size = 256
{'eval_loss': 6.168076515197754, 'eval_runtime': 1985.9527, 'eval_samples_per_second': 10.516, 'eval_steps_per_second': 0.041, 'epoch': 1.22}
{'loss': 6.1337, 'learning_rate': 1.9999999999999998e-05, 'epoch': 1.53}
 33%|██████████▋                     | 500/1500 [7:04:46<8:02:16, 28.94s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 20884
  Batch size = 256
{'eval_loss': 6.088761329650879, 'eval_runtime': 2035.3673, 'eval_samples_per_second': 10.261, 'eval_steps_per_second': 0.04, 'epoch': 1.53}
 33%|██████████▋                     | 500/1500 [7:38:41<8:02:16, 28.94s/it]Saving model checkpoint to ./tokenizer/uki/checkpoint-500
Configuration saved in ./tokenizer/uki/checkpoint-500/config.json
Model weights saved in ./tokenizer/uki/checkpoint-500/pytorch_model.bin
{'loss': 6.1118, 'learning_rate': 1.8e-05, 'epoch': 1.83}
 40%|████████████▊                   | 600/1500 [8:26:43<6:35:18, 26.35s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 20884
  Batch size = 256
{'eval_loss': 6.038704872131348, 'eval_runtime': 2016.7979, 'eval_samples_per_second': 10.355, 'eval_steps_per_second': 0.041, 'epoch': 1.83}
{'loss': 6.0767, 'learning_rate': 1.6e-05, 'epoch': 2.14}
 47%|██████████████▉                 | 700/1500 [9:48:53<7:09:49, 32.24s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 20884
  Batch size = 256
{'eval_loss': 5.983971118927002, 'eval_runtime': 1990.4684, 'eval_samples_per_second': 10.492, 'eval_steps_per_second': 0.041, 'epoch': 2.14}
{'loss': 6.011, 'learning_rate': 1.4e-05, 'epoch': 2.45}
 53%|████████████████▌              | 800/1500 [11:10:33<5:22:41, 27.66s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 20884
  Batch size = 256
{'eval_loss': 5.966508388519287, 'eval_runtime': 1964.5341, 'eval_samples_per_second': 10.631, 'eval_steps_per_second': 0.042, 'epoch': 2.45}
{'loss': 5.9928, 'learning_rate': 1.2e-05, 'epoch': 2.75}
 60%|██████████████████▌            | 900/1500 [12:32:10<4:33:55, 27.39s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 20884
  Batch size = 256
{'eval_loss': 5.937778472900391, 'eval_runtime': 2006.3199, 'eval_samples_per_second': 10.409, 'eval_steps_per_second': 0.041, 'epoch': 2.75}
{'loss': 5.9365, 'learning_rate': 9.999999999999999e-06, 'epoch': 3.06}
 67%|████████████████████          | 1000/1500 [13:55:09<3:45:57, 27.11s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 20884
  Batch size = 256
{'eval_loss': 5.902013778686523, 'eval_runtime': 2029.6892, 'eval_samples_per_second': 10.289, 'eval_steps_per_second': 0.04, 'epoch': 3.06}
 67%|████████████████████          | 1000/1500 [14:28:58<3:45:57, 27.11s/it]Saving model checkpoint to ./tokenizer/uki/checkpoint-1000
Configuration saved in ./tokenizer/uki/checkpoint-1000/config.json
Model weights saved in ./tokenizer/uki/checkpoint-1000/pytorch_model.bin
{'loss': 5.9286, 'learning_rate': 8e-06, 'epoch': 3.36}
 73%|██████████████████████        | 1100/1500 [15:20:17<2:54:44, 26.21s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 20884
  Batch size = 256
{'eval_loss': 5.890791893005371, 'eval_runtime': 2049.5299, 'eval_samples_per_second': 10.19, 'eval_steps_per_second': 0.04, 'epoch': 3.36}
{'loss': 5.9411, 'learning_rate': 6e-06, 'epoch': 3.67}
 80%|████████████████████████      | 1200/1500 [16:40:36<2:42:04, 32.41s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 20884
  Batch size = 256
{'eval_loss': 5.876568794250488, 'eval_runtime': 2024.8493, 'eval_samples_per_second': 10.314, 'eval_steps_per_second': 0.04, 'epoch': 3.67}
{'loss': 5.9383, 'learning_rate': 4e-06, 'epoch': 3.98}
 87%|██████████████████████████    | 1300/1500 [17:52:43<1:19:20, 23.80s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 20884
  Batch size = 256
{'eval_loss': 5.8730149269104, 'eval_runtime': 1599.5324, 'eval_samples_per_second': 13.056, 'eval_steps_per_second': 0.051, 'epoch': 3.98}
{'loss': 5.9195, 'learning_rate': 2e-06, 'epoch': 4.28}
 93%|█████████████████████████████▊  | 1400/1500 [18:59:02<45:12, 27.13s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 20884
  Batch size = 256
{'eval_loss': 5.862037181854248, 'eval_runtime': 1574.1125, 'eval_samples_per_second': 13.267, 'eval_steps_per_second': 0.052, 'epoch': 4.28}
{'loss': 5.9129, 'learning_rate': 0.0, 'epoch': 4.59}
100%|████████████████████████████████| 1500/1500 [20:00:38<00:00, 24.79s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 20884
  Batch size = 256
{'eval_loss': 5.866694927215576, 'eval_runtime': 1631.4292, 'eval_samples_per_second': 12.801, 'eval_steps_per_second': 0.05, 'epoch': 4.59}
100%|████████████████████████████████| 1500/1500 [20:27:50<00:00, 24.79s/it]          Saving model checkpoint to ./tokenizer/uki/checkpoint-1500
Configuration saved in ./tokenizer/uki/checkpoint-1500/config.json
Model weights saved in ./tokenizer/uki/checkpoint-1500/pytorch_model.bin


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./tokenizer/uki/checkpoint-1500 (score: 5.866694927215576).
{'train_runtime': 73673.7845, 'train_samples_per_second': 1.303, 'train_steps_per_second': 0.02, 'train_loss': 6.142794392903646, 'epoch': 4.59}
100%|████████████████████████████████| 1500/1500 [20:27:53<00:00, 49.12s/it]
The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 20884
  Batch size = 256
100%|█████████████████████████████████████████████████| 82/82 [27:23<00:00, 20.05s/it]
>>> Perplexity: 345.40
