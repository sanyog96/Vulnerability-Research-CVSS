export ENCODER_MODEL=/home1/ai21mtech12003/recycle_bert/uncased_L-12_H-768_A-12
export DECODER_MODEL=./lmn-bert
export CODE=./user_code
export CORPUS=./corpus
export DATA=./data
export MODEL_STAGE1=./model.stage1
export MODEL_STAGE2=./model.stage2
export MODEL_STAGE3=./model.stage3
export PYTHONPATH="$CODE:$PYTHONPATH"

###### eng_lmn 1 layer

source_lang=eng
target_lang=lmn
date_format=2024_06_06_03_14
model_desc_folder_name=encdecbert_nospcltoken
epoch_name=pat_500_enc12_e768_f3072_h12_dec12_e256_f1024_h12_dec1_h4_e512_f1024
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

## stage2 using stage 1 best checkpoint
source_lang=eng
target_lang=lmn
older_date_format=2024_06_06_03_14
date_format=2024_06_06_04_14
model_desc_folder_name=encdecbert_nospcltoken
epoch_name=pat_500_enc12_e768_f3072_h12_dec12_e256_f1024_h12_dec1_h4_e512_f1024
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

## stage2 using stage 1 last checkpoint
source_lang=eng
target_lang=lmn
older_date_format=2024_06_06_03_14
date_format=2024_06_06_04_16
model_desc_folder_name=encdecbert_nospcltoken
epoch_name=pat_500_enc12_e768_f3072_h12_dec12_e256_f1024_h12_dec1_h4_e512_f1024
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

## stage3 using stage 2 best checkpoint of stage1 best checkpoint
source_lang=eng
target_lang=lmn
older_date_format=2024_06_06_04_14
date_format=2024_06_06_05_14
model_desc_folder_name=encdecbert_nospcltoken
epoch_name=pat_500_enc12_e768_f3072_h12_dec12_e256_f1024_h12_dec1_h4_e512_f1024
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

## stage3 using stage 2 last checkpoint of stage1 best checkpoint
source_lang=eng
target_lang=lmn
older_date_format=2024_06_06_04_14
date_format=2024_06_06_05_16
model_desc_folder_name=encdecbert_nospcltoken
epoch_name=pat_500_enc12_e768_f3072_h12_dec12_e256_f1024_h12_dec1_h4_e512_f1024
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

#### stage3 using stage 2 best checkpoint of stage1 last checkpoint
##source_lang=eng
##target_lang=lmn
##older_date_format=2024_06_06_04_16
##date_format=2024_06_06_05_18
##model_desc_folder_name=encdecbert_nospcltoken
##epoch_name=pat_500_enc12_e768_f3072_h12_dec12_e256_f1024_h12_dec1_h4_e512_f1024
##model_desc=$model_desc_folder_name/$epoch_name
##model_folder_name=eng_lmn

#### stage3 using stage 2 last checkpoint of stage1 last checkpoint
##source_lang=eng
##target_lang=lmn
##older_date_format=2024_06_06_04_16
##date_format=2024_06_06_05_20
##model_desc_folder_name=encdecbert_nospcltoken
##epoch_name=pat_500_enc12_e768_f3072_h12_dec12_e256_f1024_h12_dec1_h4_e512_f1024
##model_desc=$model_desc_folder_name/$epoch_name
##model_folder_name=eng_lmn


#############################################################
#############################################################
#############################################################
###### eng_lmn 2 layer

source_lang=eng
target_lang=lmn
date_format=2024_06_06_06_14
model_desc_folder_name=encdecbert_nospcltoken
epoch_name=pat_500_enc12_e768_f3072_h12_dec12_e256_f1024_h12_dec2_h4_e512_f1024
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

## stage2 using stage 1 best checkpoint
source_lang=eng
target_lang=lmn
older_date_format=2024_06_06_06_14
date_format=2024_06_06_07_14
model_desc_folder_name=encdecbert_nospcltoken
epoch_name=pat_500_enc12_e768_f3072_h12_dec12_e256_f1024_h12_dec2_h4_e512_f1024
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

## stage2 using stage 1 last checkpoint
source_lang=eng
target_lang=lmn
older_date_format=2024_06_06_06_14
date_format=2024_06_06_07_16
model_desc_folder_name=encdecbert_nospcltoken
epoch_name=pat_500_enc12_e768_f3072_h12_dec12_e256_f1024_h12_dec2_h4_e512_f1024
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

## stage3 using stage 2 best checkpoint of stage1 best checkpoint
source_lang=eng
target_lang=lmn
older_date_format=2024_06_06_07_14
date_format=2024_06_06_08_14
model_desc_folder_name=encdecbert_nospcltoken
epoch_name=pat_500_enc12_e768_f3072_h12_dec12_e256_f1024_h12_dec2_h4_e512_f1024
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

## stage3 using stage 2 last checkpoint of stage1 best checkpoint
source_lang=eng
target_lang=lmn
older_date_format=2024_06_06_07_14
date_format=2024_06_06_08_16
model_desc_folder_name=encdecbert_nospcltoken
epoch_name=pat_500_enc12_e768_f3072_h12_dec12_e256_f1024_h12_dec2_h4_e512_f1024
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

#### stage3 using stage 2 best checkpoint of stage1 last checkpoint
##source_lang=eng
##target_lang=lmn
##older_date_format=2024_06_06_07_16
##date_format=2024_06_06_08_18
##model_desc_folder_name=encdecbert_nospcltoken
##epoch_name=pat_500_enc12_e768_f3072_h12_dec12_e256_f1024_h12_dec2_h4_e512_f1024
##model_desc=$model_desc_folder_name/$epoch_name
##model_folder_name=eng_lmn

#### stage3 using stage 2 last checkpoint of stage1 last checkpoint
##source_lang=eng
##target_lang=lmn
##older_date_format=2024_06_06_07_16
##date_format=2024_06_06_08_20
##model_desc_folder_name=encdecbert_nospcltoken
##epoch_name=pat_500_enc12_e768_f3072_h12_dec12_e256_f1024_h12_dec2_h4_e512_f1024
##model_desc=$model_desc_folder_name/$epoch_name
##model_folder_name=eng_lmn

python3 $CODE/preprocess.py --source-lang $source_lang --target-lang $target_lang \
--srcdict $DATA/$model_folder_name/$model_desc/dict.$source_lang\.txt \
--tgtdict $DATA/$model_folder_name/$model_desc/dict.$target_lang\.txt \
--trainpref $CORPUS/$model_folder_name/train \
--validpref $CORPUS/$model_folder_name/valid \
--testpref $CORPUS/$model_folder_name/test \
--destdir $DATA/$model_folder_name/$model_desc \
--log-file log_custom_fldr/$model_folder_name/$model_desc/preprocess_$date_format.log \
>> log_custom_fldr/$model_folder_name/$model_desc/preprocess_execution_$date_format.log

CUDA_VISIBLE_DEVICES=6 \
nohup fairseq-train \
$DATA/$model_folder_name/$model_desc \
-s $source_lang \
-t $target_lang \
--decoder-layers 1 \
--task translation_with_pretrained_model \
--user-dir $CODE \
--encoder-model $ENCODER_MODEL \
--decoder-model $DECODER_MODEL \
--arch transformer_with_pretrained_model_512 \
--log-format simple --max-update 1000000 \
--share-decoder-input-output-embed \
--optimizer adam --adam-betas '(0.9, 0.99)' \
--warmup-init-lr 1e-07 --lr 5e-4 \
--lr-scheduler inverse_sqrt --warmup-updates 4000 \
--weight-decay 0.001 --criterion label_smoothed_cross_entropy \
--label-smoothing 0.5 --ddp-backend legacy_ddp --clip-norm 1.0 \
--patience 500 --batch-size 4096 --max-tokens 4096 \
--no-epoch-checkpoints --skip-invalid-size-inputs-valid-test \
--fp16 --save-dir $MODEL_STAGE1/$model_folder_name/$model_desc/$date_format \
--wandb-project $model_folder_name\_stage1_$model_desc_folder_name\_$epoch_name \
--log-file log_custom_fldr/$model_folder_name/$model_desc/train_stage1_$date_format.log \
>> log_custom_fldr/$model_folder_name/$model_desc/train_execution_stage1_$date_format.log 2>&1 &

CUDA_VISIBLE_DEVICES=5 \
nohup fairseq-train \
$DATA/$model_folder_name/$model_desc \
-s $source_lang \
-t $target_lang \
--decoder-layers 1 \
--task translation_with_pretrained_model \
--user-dir $CODE \
--encoder-model $ENCODER_MODEL \
--decoder-model $DECODER_MODEL \
--arch transformer_with_pretrained_model_512 \
--decoder-fine-tuning \
--restore-file /home1/ai21mtech12003/encoder_decoder/$MODEL_STAGE1\/$model_folder_name\/$model_desc\/$older_date_format\/checkpoint_best.pt \
--reset-lr-scheduler --reset-meters --reset-optimizer \
--log-format simple \
--share-decoder-input-output-embed \
--optimizer adam --adam-betas '(0.9, 0.99)' \
--warmup-init-lr 1e-07 --lr 4e-4 \
--lr-scheduler inverse_sqrt --warmup-updates 1000 \
--weight-decay 0.001 --criterion label_smoothed_cross_entropy \
--label-smoothing 0.5 --ddp-backend legacy_ddp --clip-norm 1 \
--patience 500 --batch-size 4096 --max-tokens 4096 \
--no-epoch-checkpoints --skip-invalid-size-inputs-valid-test \
--wandb-project $model_folder_name\_stage2_$model_desc_folder_name\_$epoch_name \
--fp16 --save-dir $MODEL_STAGE2/$model_folder_name/$model_desc/$date_format \
--log-file log_custom_fldr/$model_folder_name/$model_desc/train_stage2_$date_format.log \
>> log_custom_fldr/$model_folder_name/$model_desc/train_execution_stage2_$date_format.log 2>&1 &

CUDA_VISIBLE_DEVICES=5 \
nohup fairseq-train \
$DATA/$model_folder_name/$model_desc \
-s $source_lang \
-t $target_lang \
--decoder-layers 1 \
--task translation_with_pretrained_model \
--user-dir $CODE \
--encoder-model $ENCODER_MODEL \
--decoder-model $DECODER_MODEL \
--arch transformer_with_pretrained_model_512 \
--encoder-fine-tuning \
--decoder-fine-tuning \
--restore-file /home1/ai21mtech12003/encoder_decoder/$MODEL_STAGE2\/$model_folder_name\/$model_desc\/$older_date_format\/checkpoint_best.pt \
--reset-lr-scheduler --reset-meters --reset-optimizer \
--log-format simple \
--share-decoder-input-output-embed \
--optimizer adam --adam-betas '(0.9, 0.99)' \
--warmup-init-lr 1e-07 --lr 4e-4 \
--lr-scheduler inverse_sqrt --warmup-updates 1000 \
--weight-decay 0.001 --criterion label_smoothed_cross_entropy \
--label-smoothing 0.5 --ddp-backend legacy_ddp --clip-norm 1 \
--patience 300 --batch-size 4096 --max-tokens 4096 \
--no-epoch-checkpoints --skip-invalid-size-inputs-valid-test \
--wandb-project $model_folder_name\_stage3_$model_desc_folder_name\_$epoch_name \
--fp16 --save-dir $MODEL_STAGE3/$model_folder_name/$model_desc/$date_format \
--log-file log_custom_fldr/$model_folder_name/$model_desc/train_stage3_$date_format.log \
>> log_custom_fldr/$model_folder_name/$model_desc/train_execution_stage3_$date_format.log 2>&1 &

bash generate_output_bert.sh -n $model_folder_name -m $ENCODER_MODEL -q $DECODER_MODEL -u $CODE -k translation_with_pretrained_model -d $model_desc -s $source_lang -t $target_lang -a 5 -f $date_format -c best -b stage2