## eng-lmn
export BERT_MODEL=../recycle_bert/uncased_L-12_H-768_A-12
export CODE=./user_code
export CORPUS=./corpus
export DATA=./data-bin
export MODEL_STAGE1=./model.stage1
export MODEL_STAGE2=./model.stage2

python3 $CODE/preprocess.py --source-lang $source_lang --target-lang $target_lang \
--srcdict $DATA/$model_folder_name/$model_desc/dict.$source_lang\.txt \
--trainpref ../ssmt_dataset/$model_folder_name\/train \
--validpref ../ssmt_dataset/$model_folder_name\/valid \
--testpref ../ssmt_dataset/$model_folder_name\/test \
--destdir $DATA/$model_folder_name/$model_desc \
--log-file log_custom_fldr/$model_folder_name/$model_desc/preprocess_$date_format.log \
>> log_custom_fldr/$model_folder_name/$model_desc/preprocess_execution_$date_format.log

fairseq-preprocess --source-lang $source_lang --target-lang $target_lang \
--srcdict $DATA/$model_folder_name/$model_desc/dict.$source_lang\.txt \
--dict-only \
--trainpref ../ssmt_dataset/$model_folder_name\/train \
--validpref ../ssmt_dataset/$model_folder_name\/valid \
--testpref ../ssmt_dataset/$model_folder_name\/test \
--destdir $DATA/$model_folder_name/$model_desc \
--workers 20 --log-file log_custom_fldr/$model_folder_name/$model_desc/preprocess_$date_format.log \
>> log_custom_fldr/$model_folder_name/$model_desc/preprocess_execution_$date_format.log

CUDA_VISIBLE_DEVICES=3 \
nohup fairseq-train \
$DATA/$model_folder_name/$model_desc\/ \
-s $source_lang \
-t $target_lang \
--task subword_segmental_translation \
--model $BERT_MODEL \
--model-name bert \
--arch ssmt_bert \
--log-format simple --max-update=1000000 \
--share-decoder-input-output-embed \
--optimizer adam --adam-betas '(0.9, 0.99)' \
--warmup-init-lr 1e-07 --lr 5e-4 \
--lr-scheduler inverse_sqrt --warmup-updates 4000 \
--vocabs-path $DATA/$model_folder_name\/$model_desc \
--weight-decay 0.001 --criterion subword_segmental_cross_entropy \
--label-smoothing 0.5 --ddp-backend legacy_ddp --clip-norm 1.0 \
--patience 500 --batch-size 4096 --max-tokens 4096 --no-epoch-checkpoints \
--skip-invalid-size-inputs-valid-test --max-seg-len 5 --lexicon-max-size 2500 \
--wandb-project $model_folder_name\_stage1_$model_desc_folder_name\_$epoch_name \
--fp16 --save-dir $MODEL_STAGE1/$model_folder_name/$model_desc/$date_format \
--log-file log_custom_fldr/$model_folder_name/$model_desc/train_$date_format.log \
>> log_custom_fldr/$model_folder_name/$model_desc/train_execution_$date_format.log 2>&1 &

CUDA_VISIBLE_DEVICES=6 \
nohup fairseq-train \
$DATA/$model_folder_name/$model_desc\/ \
-s $source_lang \
-t $target_lang \
--task subword_segmental_translation \
--model $BERT_MODEL \
--model-name bert \
--arch ssmt_bert \
--fine-tuning \
--restore-file /home1/ai21mtech12003/fairseq/model.stage1/$model_folder_name\/$model_desc\/$older_date_format\/checkpoint_best.pt \
--reset-lr-scheduler --reset-meters --reset-optimizer \
--log-format simple --max-update=1000000 \
--share-decoder-input-output-embed \
--optimizer adam --adam-betas '(0.9, 0.99)' \
--warmup-init-lr 1e-07 --lr 2e-4 \
--lr-scheduler inverse_sqrt --warmup-updates 1000 \
--vocabs-path $DATA/$model_folder_name\/$model_desc\/ \
--weight-decay 0.001 --criterion subword_segmental_cross_entropy \
--label-smoothing 0.5 --ddp-backend legacy_ddp --clip-norm 1 \
--patience 500 --batch-size 4096 --max-tokens 4096 --no-epoch-checkpoints \
--skip-invalid-size-inputs-valid-test --max-seg-len 5 --lexicon-max-size 2500 \
--wandb-project $model_folder_name\_stage2_$model_desc_folder_name\_$epoch_name \
--fp16 --save-dir $MODEL_STAGE2/$model_folder_name/$model_desc/$date_format \
--log-file log_custom_fldr/$model_folder_name/$model_desc/train_stage2_$date_format.log \
>> log_custom_fldr/$model_folder_name/$model_desc/train_execution_stage2_$date_format.log 2>&1 &

CUDA_VISIBLE_DEVICES=$cuda_device python3 fairseq_cli/generate_ssmt.py data-bin/$model_folder_name/$model_desc/ --model $bert_model --model-name "bert" --source-lang $source_lang --target-lang $target_lang --task $task --log-file log_custom_fldr/$model_folder_name/$model_desc/generate_test_$date_format.log --path $dir/$model_folder_name/$model_desc/$date_format/checkpoint_$checkpoint.pt --max-len-b 500 --vocabs-path data-bin/$model_folder_name/$model_desc/ --sacrebleu --normalize-type seg-seg --max-seg-len 5 --lexicon-max-size 5000 --batch-size 64 --remove-bpe --skip-invalid-size-inputs-valid-test --beam 5 --gen-subset test --results-path output_custom_fldr/$model_folder_name/$model_desc/$date_format/$checkpoint/ 
 >> log_custom_fldr/$model_folder_name/$model_desc/generate_test_execution_$date_format.log 2>&1 &
 
CUDA_VISIBLE_DEVICES=$cuda_device fairseq-generate data/$model_folder_name/$model_desc \
--model $bert_model --model-name "bert" -s $source_lang -t $target_lang \
--user-dir $user_dir --task $task \
--sacrebleu --path  \
--batch-size 64 --beam 5 --gen-subset test \
--remove-bpe --skip-invalid-size-inputs-valid-test \
> output_custom_fldr/$model_folder_name/$model_desc/$date_format/$checkpoint/output_test_$lang_$date_format.txt

bash generate_output.sh -n $model_folder_name -m $BERT_MODEL -u $CODE -k subword_segmental_translation -d $model_desc -s $source_lang -t $target_lang -a 5 -f $date_format -c best -b stage2

source_lang=eng
target_lang=lmn
date_format=2024_05_25_18_33
model_desc_folder_name=ssmt_conf
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

source_lang=eng
target_lang=lmn
older_date_format=2024_05_25_18_33
date_format=2024_05_25_18_50
model_desc_folder_name=ssmt_conf
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

### Eng-sle

source_lang=eng
target_lang=sle
date_format=2024_05_25_21_47
model_desc_folder_name=ssmt_conf
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_sle

source_lang=eng
target_lang=sle
older_date_format=2024_05_25_21_47
date_format=2024_05_25_21_53
model_desc_folder_name=ssmt_conf
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_sle

### Eng-uki

source_lang=eng
target_lang=uki
date_format=2024_05_25_22_50
model_desc_folder_name=ssmt_conf
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_uki

source_lang=eng
target_lang=uki
older_date_format=2024_05_25_22_50
date_format=2024_05_25_22_55
model_desc_folder_name=ssmt_conf
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_uki

### Eng-unr

source_lang=eng
target_lang=unr
date_format=2024_05_25_23_12
model_desc_folder_name=ssmt_conf
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_unr

source_lang=eng
target_lang=unr
older_date_format=2024_05_25_23_12
date_format=2024_05_25_23_15
model_desc_folder_name=ssmt_conf
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_unr

### eng_10K-hin_10K

source_lang=eng_10K
target_lang=hin_10K
date_format=2024_05_25_23_30
model_desc_folder_name=ssmt_conf
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_hin_10K

source_lang=eng_10K
target_lang=hin_10K
older_date_format=2024_05_25_23_30
date_format=2024_05_25_23_37
model_desc_folder_name=ssmt_conf
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_hin_10K



#######################################################################
#######################################################################
#######################################################################
#### adding label_smoothed_cross_entropy function with subword segmental loss

export BERT_MODEL=../recycle_bert/uncased_L-12_H-768_A-12
export CODE=./user_code
export CORPUS=./corpus
export DATA=./data-bin
export MODEL_STAGE1=./model.stage1
export MODEL_STAGE2=./model.stage2

### eng_lmn
source_lang=eng
target_lang=lmn
date_format=2024_06_05_00_39
model_desc_folder_name=ssmt_lsce
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

# stage 2 from stage 1 best checkpoint
source_lang=eng
target_lang=lmn
older_date_format=2024_06_05_00_39
date_format=2024_06_05_14_39
model_desc_folder_name=ssmt_lsce
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

# stage 2 from stage 1 last checkpoint
source_lang=eng
target_lang=lmn
older_date_format=2024_06_05_00_39
date_format=2024_06_05_14_41
model_desc_folder_name=ssmt_lsce
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

### eng_10K-hin_10K

source_lang=eng_10K
target_lang=hin_10K
date_format=2024_06_05_00_51
model_desc_folder_name=ssmt_lsce
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_hin_10K

source_lang=eng_10K
target_lang=hin_10K
older_date_format=2024_06_05_00_51
date_format=2024_06_05_01_51
model_desc_folder_name=ssmt_lsce
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_hin_10K

source_lang=eng_10K
target_lang=hin_10K
older_date_format=2024_06_05_00_51
date_format=2024_06_05_01_53
model_desc_folder_name=ssmt_lsce
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_hin_10K


#######################################################
#######################################################
#######################################################
### 2 layers at Char LSTM part of decoder

export BERT_MODEL=../recycle_bert/uncased_L-12_H-768_A-12
export CODE=./user_code
export CORPUS=./corpus
export DATA=./data-bin
export MODEL_STAGE1=./model.stage1
export MODEL_STAGE2=./model.stage2

### eng_lmn
source_lang=eng
target_lang=lmn
date_format=2024_06_07_00_39
model_desc_folder_name=ssmt_lsce2
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

# stage 2 from stage 1 best checkpoint
source_lang=eng
target_lang=lmn
older_date_format=2024_06_07_00_39
date_format=2024_06_07_01_39
model_desc_folder_name=ssmt_lsce2
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

# stage 2 from stage 1 last checkpoint
source_lang=eng
target_lang=lmn
older_date_format=2024_06_07_00_39
date_format=2024_06_07_01_41
model_desc_folder_name=ssmt_lsce2
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

### eng_10K-hin_10K

source_lang=eng_10K
target_lang=hin_10K
date_format=2024_06_11_00_51
model_desc_folder_name=ssmt_lsce2
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_hin_10K

source_lang=eng_10K
target_lang=hin_10K
older_date_format=2024_06_11_00_51
date_format=2024_06_11_01_51
model_desc_folder_name=ssmt_lsce2
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_hin_10K

source_lang=eng_10K
target_lang=hin_10K
older_date_format=2024_06_11_00_51
date_format=2024_06_11_01_53
model_desc_folder_name=ssmt_lsce2
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_hin_10K

### 3 layers at Char LSTM part of decoder

export BERT_MODEL=../recycle_bert/uncased_L-12_H-768_A-12
export CODE=./user_code
export CORPUS=./corpus
export DATA=./data-bin
export MODEL_STAGE1=./model.stage1
export MODEL_STAGE2=./model.stage2

### eng_lmn
source_lang=eng
target_lang=lmn
date_format=2024_06_07_03_39
model_desc_folder_name=ssmt_lsce3
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

# stage 2 from stage 1 best checkpoint
source_lang=eng
target_lang=lmn
older_date_format=2024_06_07_03_39
date_format=2024_06_07_04_39
model_desc_folder_name=ssmt_lsce3
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

# stage 2 from stage 1 last checkpoint
source_lang=eng
target_lang=lmn
older_date_format=2024_06_07_03_39
date_format=2024_06_07_04_41
model_desc_folder_name=ssmt_lsce3
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

CUDA_VISIBLE_DEVICES=7 \
nohup fairseq-train \
$DATA/$model_folder_name/$model_desc\/ \
-s $source_lang \
-t $target_lang \
--task subword_segmental_translation \
--model $BERT_MODEL \
--model-name bert \
--arch ssmt_bert \
--charlstmlayers 2 \
--log-format simple --max-update=1000000 \
--share-decoder-input-output-embed \
--optimizer adam --adam-betas '(0.9, 0.99)' \
--warmup-init-lr 1e-07 --lr 5e-4 \
--lr-scheduler inverse_sqrt --warmup-updates 4000 \
--vocabs-path $DATA/$model_folder_name\/$model_desc \
--weight-decay 0.001 --criterion subword_segmental_cross_entropy \
--label-smoothing 0.5 --ddp-backend legacy_ddp --clip-norm 1.0 \
--patience 400 --batch-size 4096 --max-tokens 4096 --no-epoch-checkpoints \
--skip-invalid-size-inputs-valid-test --max-seg-len 5 --lexicon-max-size 2500 \
--wandb-project $model_folder_name\_stage1_$model_desc_folder_name\_$epoch_name \
--fp16 --save-dir $MODEL_STAGE1/$model_folder_name/$model_desc/$date_format \
--log-file log_custom_fldr/$model_folder_name/$model_desc/train_$date_format.log \
>> log_custom_fldr/$model_folder_name/$model_desc/train_execution_$date_format.log 2>&1 &

CUDA_VISIBLE_DEVICES=0 \
nohup fairseq-train \
$DATA/$model_folder_name/$model_desc\/ \
-s $source_lang \
-t $target_lang \
--charlstmlayers 1 \
--task subword_segmental_translation \
--model $BERT_MODEL \
--model-name bert \
--arch ssmt_bert \
--fine-tuning \
--restore-file /home1/ai21mtech12003/fairseq/model.stage1/$model_folder_name\/$model_desc\/$older_date_format\/checkpoint_best.pt \
--reset-lr-scheduler --reset-meters --reset-optimizer \
--log-format simple --max-update=1000000 \
--share-decoder-input-output-embed \
--optimizer adam --adam-betas '(0.9, 0.99)' \
--warmup-init-lr 1e-07 --lr 2e-4 \
--lr-scheduler inverse_sqrt --warmup-updates 1000 \
--vocabs-path $DATA/$model_folder_name\/$model_desc\/ \
--weight-decay 0.001 --criterion subword_segmental_cross_entropy \
--label-smoothing 0.5 --ddp-backend legacy_ddp --clip-norm 1 \
--patience 200 --batch-size 4096 --max-tokens 4096 --no-epoch-checkpoints \
--skip-invalid-size-inputs-valid-test --max-seg-len 5 --lexicon-max-size 2500 \
--wandb-project $model_folder_name\_stage2_$model_desc_folder_name\_$epoch_name \
--fp16 --save-dir $MODEL_STAGE2/$model_folder_name/$model_desc/$date_format \
--log-file log_custom_fldr/$model_folder_name/$model_desc/train_stage2_$date_format.log \
>> log_custom_fldr/$model_folder_name/$model_desc/train_execution_stage2_$date_format.log 2>&1 &

bash generate_output.sh -n $model_folder_name -m $BERT_MODEL -u $CODE -k subword_segmental_translation -d $model_desc -s $source_lang -t $target_lang -a 5 -f $date_format -c best -b stage2



#######################################################################
#######################################################################
#######################################################################
#### 1 layer of ChartransformerEncoder adding label_smoothed_cross_entropy function 
#### with subword segmental loss

export BERT_MODEL=../recycle_bert/uncased_L-12_H-768_A-12
export CODE=./user_code
export CORPUS=./corpus
export DATA=./data-bin
export MODEL_STAGE1=./model.stage1
export MODEL_STAGE2=./model.stage2

### eng_lmn
source_lang=eng
target_lang=lmn
date_format=2024_06_08_00_39
model_desc_folder_name=ssmt_lsce_te1
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn


#######################################################
#######################################################
#######################################################
### 2 layers at Char LSTM part of decoder

export BERT_MODEL=../recycle_bert/uncased_L-12_H-768_A-12
export CODE=./user_code
export CORPUS=./corpus
export DATA=./data-bin
export MODEL_STAGE1=./model.stage1
export MODEL_STAGE2=./model.stage2

### eng_lmn
source_lang=eng
target_lang=lmn
date_format=2024_06_08_04_39
model_desc_folder_name=ssmt_lsce_te2
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn

### 3 layers at Char LSTM part of decoder

export BERT_MODEL=../recycle_bert/uncased_L-12_H-768_A-12
export CODE=./user_code
export CORPUS=./corpus
export DATA=./data-bin
export MODEL_STAGE1=./model.stage1
export MODEL_STAGE2=./model.stage2

### eng_lmn
source_lang=eng
target_lang=lmn
date_format=2024_06_08_08_39
model_desc_folder_name=ssmt_lsce_te3
epoch_name=pat_500_enc12_e768_f3072_h12_dec5_e512_f1024_h4
model_desc=$model_desc_folder_name/$epoch_name
model_folder_name=eng_lmn