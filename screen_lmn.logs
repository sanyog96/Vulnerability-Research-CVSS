26912708
max_steps is given, it will override any value given in num_train_epochs
The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expecte
d by `BertForMaskedLM.forward`,  you can safely ignore this message.
/home/sanyog/anaconda3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use
the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 19787
  Num Epochs = 5
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 1500
  Number of trainable parameters = 26912708
{'loss': 7.186, 'learning_rate': 2.8e-05, 'epoch': 0.32}
  7%|██                             | 100/1500 [1:04:31<15:10:44, 39.03s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and
have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 19787
  Batch size = 256
{'eval_loss': 6.88826847076416, 'eval_runtime': 2651.753, 'eval_samples_per_second': 7.462, 'eval_steps_per_second': 0.029, 'epoch': 0.32}
{'loss': 6.8232, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.65}
 13%|████▏                          | 200/1500 [2:56:43<14:15:43, 39.50s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and
have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 19787
  Batch size = 256
{'eval_loss': 6.649072647094727, 'eval_runtime': 2629.5183, 'eval_samples_per_second': 7.525, 'eval_steps_per_second': 0.03, 'epoch': 0.65}
{'loss': 6.6436, 'learning_rate': 2.4e-05, 'epoch': 0.97}
 20%|██████▏                        | 300/1500 [4:49:35<14:01:21, 42.07s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and
have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 19787
  Batch size = 256
{'eval_loss': 6.559828758239746, 'eval_runtime': 2734.2814, 'eval_samples_per_second': 7.237, 'eval_steps_per_second': 0.029, 'epoch': 0.97}
{'loss': 6.5133, 'learning_rate': 2.2e-05, 'epoch': 1.29}
 27%|████████▎                      | 400/1500 [6:39:06<13:04:38, 42.80s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and
have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 19787
  Batch size = 256
{'eval_loss': 6.500583171844482, 'eval_runtime': 2006.5146, 'eval_samples_per_second': 9.861, 'eval_steps_per_second': 0.039, 'epoch': 1.29}
{'loss': 6.511, 'learning_rate': 1.9999999999999998e-05, 'epoch': 1.61}
 33%|██████████▎                    | 500/1500 [8:17:48<10:22:53, 37.37s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 19787
  Batch size = 256
{'eval_loss': 6.439080715179443, 'eval_runtime': 2078.718, 'eval_samples_per_second': 9.519, 'eval_steps_per_second': 0.038, 'epoch': 1.61}
 33%|██████████▎                    | 500/1500 [8:52:26<10:22:53, 37.37s/it]Saving model checkpoint to ./tokenizer/lmn/checkpoint-500
Configuration saved in ./tokenizer/lmn/checkpoint-500/config.json
Model weights saved in ./tokenizer/lmn/checkpoint-500/pytorch_model.bin
{'loss': 6.4677, 'learning_rate': 1.8e-05, 'epoch': 1.94}
 40%|████████████▍                  | 600/1500 [10:00:09<9:47:59, 39.20s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 19787
  Batch size = 256
{'eval_loss': 6.365198135375977, 'eval_runtime': 2053.8121, 'eval_samples_per_second': 9.634, 'eval_steps_per_second': 0.038, 'epoch': 1.94}
{'loss': 6.363, 'learning_rate': 1.6e-05, 'epoch': 2.26}
 47%|██████████████▍                | 700/1500 [11:40:36<9:20:10, 42.01s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 19787
  Batch size = 256
{'eval_loss': 6.314032077789307, 'eval_runtime': 2032.7679, 'eval_samples_per_second': 9.734, 'eval_steps_per_second': 0.038, 'epoch': 2.26}
{'loss': 6.3504, 'learning_rate': 1.4e-05, 'epoch': 2.58}
 53%|████████████████▌              | 800/1500 [13:15:48<6:38:50, 34.19s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 19787
  Batch size = 256
{'eval_loss': 6.286144256591797, 'eval_runtime': 2001.6761, 'eval_samples_per_second': 9.885, 'eval_steps_per_second': 0.039, 'epoch': 2.58}
{'loss': 6.3143, 'learning_rate': 1.2e-05, 'epoch': 2.9}
 60%|██████████████████▌            | 900/1500 [14:52:17<6:25:51, 38.59s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 19787
  Batch size = 256
{'eval_loss': 6.229752540588379, 'eval_runtime': 1990.5521, 'eval_samples_per_second': 9.94, 'eval_steps_per_second': 0.039, 'epoch': 2.9}
{'loss': 6.251, 'learning_rate': 9.999999999999999e-06, 'epoch': 3.23}
 67%|████████████████████          | 1000/1500 [16:28:48<5:23:10, 38.78s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 19787
  Batch size = 256
{'eval_loss': 6.209071159362793, 'eval_runtime': 2046.6882, 'eval_samples_per_second': 9.668, 'eval_steps_per_second': 0.038, 'epoch': 3.23}
 67%|████████████████████          | 1000/1500 [17:02:55<5:23:10, 38.78s/it]Saving model checkpoint to ./tokenizer/lmn/checkpoint-1000
Configuration saved in ./tokenizer/lmn/checkpoint-1000/config.json
Model weights saved in ./tokenizer/lmn/checkpoint-1000/pytorch_model.bin
{'loss': 6.2459, 'learning_rate': 8e-06, 'epoch': 3.55}
 73%|██████████████████████        | 1100/1500 [17:51:54<3:12:38, 28.90s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 19787
  Batch size = 256
{'eval_loss': 6.158289909362793, 'eval_runtime': 1649.3268, 'eval_samples_per_second': 11.997, 'eval_steps_per_second': 0.047, 'epoch': 3.55}
{'loss': 6.239, 'learning_rate': 6e-06, 'epoch': 3.87}
 80%|████████████████████████      | 1200/1500 [19:05:00<2:02:29, 24.50s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 19787
  Batch size = 256
{'eval_loss': 6.153285026550293, 'eval_runtime': 1572.5718, 'eval_samples_per_second': 12.583, 'eval_steps_per_second': 0.05, 'epoch': 3.87}
{'loss': 6.2199, 'learning_rate': 4e-06, 'epoch': 4.19}
 87%|██████████████████████████    | 1300/1500 [20:12:51<1:34:29, 28.35s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 19787
  Batch size = 256
{'eval_loss': 6.153389930725098, 'eval_runtime': 1593.7123, 'eval_samples_per_second': 12.416, 'eval_steps_per_second': 0.049, 'epoch': 4.19}
{'loss': 6.1957, 'learning_rate': 2e-06, 'epoch': 4.52}
 93%|█████████████████████████████▊  | 1400/1500 [21:09:29<21:50, 13.10s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 19787
  Batch size = 256
{'eval_loss': 6.143764019012451, 'eval_runtime': 965.9442, 'eval_samples_per_second': 20.485, 'eval_steps_per_second': 0.081, 'epoch': 4.52}
{'loss': 6.192, 'learning_rate': 0.0, 'epoch': 4.84}
100%|████████████████████████████████| 1500/1500 [21:47:47<00:00, 12.63s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 19787
  Batch size = 256
{'eval_loss': 6.138312816619873, 'eval_runtime': 942.7576, 'eval_samples_per_second': 20.988, 'eval_steps_per_second': 0.083, 'epoch': 4.84}
100%|████████████████████████████████| 1500/1500 [22:03:29<00:00, 12.63s/it]          Saving model checkpoint to ./tokenizer/lmn/checkpoint-1500
Configuration saved in ./tokenizer/lmn/checkpoint-1500/config.json
Model weights saved in ./tokenizer/lmn/checkpoint-1500/pytorch_model.bin


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./tokenizer/lmn/checkpoint-1500 (score: 6.138312816619873).
{'train_runtime': 79410.6653, 'train_samples_per_second': 1.209, 'train_steps_per_second': 0.019, 'train_loss': 6.434404541015625, 'epoch': 4.84}
100%|████████████████████████████████| 1500/1500 [22:03:30<00:00, 52.94s/it]
The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 19787
  Batch size = 256
100%|█████████████████████████████████████████████████| 78/78 [15:04<00:00, 11.60s/it]

>>> Perplexity: 463.94
